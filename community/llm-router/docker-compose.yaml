services:
  model-server:
    build:
      context: .
      dockerfile: docker/triton.dockerfile
    image: router-models:latest
    shm_size: 8G
    network_mode: "host"
    ulimits:
      memlock: -1
      stack: 67108864
    volumes:
      - ${PWD}/model_repository:/model_repository
    command: tritonserver --log-verbose=1 --model-repository=/model_repository --model-control-mode=explicit --load-model=task_router_ensemble
    ports:
      - "8000:8000"
      - "8001:8001"
      - "8002:8002"
    deploy:
      resources:
        reservations:
          devices:
            - driver: nvidia
              device_ids: [ "0" ]
              capabilities: [ gpu ]

  app:
    build:
      context: .
      dockerfile: docker/app.dockerfile
    image: nim-thin-client:app
    working_dir: /app
    ports:
      - 8008:8008
    volumes:
      - type: bind
        source: app
        target: /app
    command: python app.py

  router-controller:
    build:
      context: .
      dockerfile: docker/router_controller.dockerfile
    image: router-controller:latest
    working_dir: /router_controller
    volumes:
      - type: bind
        source: router_controller
        target: /router_controller
    command: cargo watch -q -c -w ./crates/llm-router-gateway-api -x test -x "run -q --bin llm-router-gateway-api -- --config-path ./config.yaml"
    ports:
      - "8084:8084"
